{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMzoZoUsxpc2CXaNUG4bsWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesstaub/Listen-Up/blob/main/audio_slicer_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxOJbneh3RMw",
        "outputId": "677151c2-1579-431c-94c8-70084507e889"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.6 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title 1. Install Dependencies & Import Libraries\n",
        "!pip install -q umap-learn\n",
        "!pip install -q librosa soundfile ipywidgets\n",
        "\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.cluster import KMeans\n",
        "import umap\n",
        "from scipy.spatial.distance import cdist\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# @title 2. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"‚úÖ Drive Mounted.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuVvHi_3coj",
        "outputId": "c413c469-7304-404d-a1cf-79c70c09bc06"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Drive Mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONFIG = {\n",
        "    # ---- paths ----\n",
        "    \"input_folder\": \"/content/drive/My Drive/audio/sp-tools-corpora/preparedpiano\",\n",
        "    \"output_folder\": \"/content/drive/My Drive/audio/sp-tools-corpora/preparedpiano/clusters\",\n",
        "    \"data_file\": \"/content/drive/My Drive/audio/sp-tools-corpora/preparedpiano/clusters/analysis_data.csv\",\n",
        "\n",
        "    # ---- behavior ----\n",
        "    \"resume\": True,        # load existing analysis if present\n",
        "    \"clear_existing\": False,  # nuke data + clusters before run\n",
        "\n",
        "    # ---- audio ----\n",
        "    \"sample_rate\": 22050,\n",
        "    \"stereo_mode\": \"sum\",\n",
        "    \"file_limit\": 0,\n",
        "\n",
        "    \"min_duration\": 0.1,\n",
        "    \"max_duration\": 4.0,\n",
        "\n",
        "    # ---- clustering ----\n",
        "    \"n_clusters\": 8,\n",
        "\n",
        "    # ---- filtering ----\n",
        "    \"similarity_threshold\": 0.6, # 0.95 -> keeps most slices, 0.05 -> keep only the most unique slices\n",
        "\n",
        "    \"umap\": {\n",
        "        \"n_neighbors\": 15,\n",
        "        \"min_dist\": 0.1\n",
        "    },\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "jj6l092DIBpx"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def feature_columns(self, descriptor):\n",
        "    if descriptor == \"timbral\":\n",
        "        return [\n",
        "            c for c in self.df.columns\n",
        "            if c.startswith(\"mfcc_\") or c.startswith(\"mfcc_delta_\")\n",
        "        ]\n",
        "    elif descriptor == \"tonal\":\n",
        "        return [\n",
        "            c for c in self.df.columns\n",
        "            if c.startswith(\"chroma_\") or c.startswith(\"chroma_delta_\")\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown descriptor: {descriptor}\")\n",
        "\n",
        "def stats(x):\n",
        "    return {\n",
        "        \"mean\": float(np.mean(x)),\n",
        "        \"min\": float(np.min(x)),\n",
        "        \"max\": float(np.max(x)),\n",
        "        \"std\": float(np.std(x))\n",
        "    }\n",
        "\n",
        "def clear_existing_state(cfg):\n",
        "    if os.path.exists(cfg[\"data_file\"]):\n",
        "        print(\"üßπ Removing existing analysis data\")\n",
        "        os.remove(cfg[\"data_file\"])\n",
        "\n",
        "    if os.path.exists(cfg[\"output_folder\"]):\n",
        "        print(\"üßπ Removing existing cluster folders\")\n",
        "        for root, dirs, files in os.walk(cfg[\"output_folder\"], topdown=False):\n",
        "            for f in files:\n",
        "                os.remove(os.path.join(root, f))\n",
        "            for d in dirs:\n",
        "                os.rmdir(os.path.join(root, d))"
      ],
      "metadata": {
        "id": "2Evc6rJVc1k5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class SampleBrain:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        self.sr = cfg[\"sample_rate\"]\n",
        "        self.df = pd.DataFrame()\n",
        "        self.features = None\n",
        "\n",
        "    def find_zero_crossing(self, y, idx):\n",
        "        if idx <= 0 or idx >= len(y) - 1:\n",
        "            return idx\n",
        "        win = int(0.02 * self.sr)\n",
        "        s = max(0, idx - win)\n",
        "        e = min(len(y), idx + win)\n",
        "        zc = np.where(np.diff(np.signbit(y[s:e])))[0]\n",
        "        if len(zc):\n",
        "            return s + zc[np.argmin(np.abs(zc - (idx - s)))]\n",
        "        return idx\n",
        "\n",
        "    def apply_envelope(self, y, fade_ms=5):\n",
        "        n = int((fade_ms / 1000) * self.sr)\n",
        "        if len(y) < 2 * n:\n",
        "            return y\n",
        "        env = np.ones(len(y))\n",
        "        env[:n] = np.linspace(0, 1, n)\n",
        "        env[-n:] = np.linspace(1, 0, n)\n",
        "        return y * env\n",
        "\n",
        "    def analyze(self):\n",
        "        if self.cfg.get(\"resume\") and os.path.exists(self.cfg[\"data_file\"]):\n",
        "            self.load_dataframe()\n",
        "            print(f\"‚úÖ Loaded {len(self.df)} existing slices\")\n",
        "            return\n",
        "\n",
        "        cfg = self.cfg\n",
        "\n",
        "        # --- gather audio files ---\n",
        "        files = []\n",
        "        for ext in (\"wav\", \"mp3\", \"aiff\"):\n",
        "            files += glob.glob(os.path.join(cfg[\"input_folder\"], \"**\", f\"*.{ext}\"), recursive=True)\n",
        "        if cfg.get(\"file_limit\"):\n",
        "            files = files[:cfg[\"file_limit\"]]\n",
        "\n",
        "        print(f\"üìÇ Found {len(files)} audio files\")\n",
        "        rows = []\n",
        "\n",
        "        # --- process files ---\n",
        "        for i, path in enumerate(files):\n",
        "            print(f\"üîç [{i+1}/{len(files)}] {os.path.basename(path)}\")\n",
        "            try:\n",
        "                y, _ = librosa.load(path, sr=self.sr, mono=False)\n",
        "\n",
        "                # handle channels\n",
        "                channels = []\n",
        "                if y.ndim == 1:\n",
        "                    channels = [(\"mono\", y)]\n",
        "                else:\n",
        "                    if cfg[\"stereo_mode\"] == \"sum\":\n",
        "                        channels = [(\"sum\", librosa.to_mono(y))]\n",
        "                    elif cfg[\"stereo_mode\"] == \"split\":\n",
        "                        channels = [(\"L\", y[0]), (\"R\", y[1])]\n",
        "                    elif cfg[\"stereo_mode\"] == \"L\":\n",
        "                        channels = [(\"L\", y[0])]\n",
        "                    elif cfg[\"stereo_mode\"] == \"R\":\n",
        "                        channels = [(\"R\", y[1])]\n",
        "\n",
        "                for ch, sig in channels:\n",
        "                    onsets = librosa.onset.onset_detect(y=sig, sr=self.sr, units=\"samples\")\n",
        "                    if len(onsets) == 0:\n",
        "                        onsets = [0]\n",
        "\n",
        "                    for j in range(len(onsets)):\n",
        "                        s = onsets[j]\n",
        "                        e = onsets[j + 1] if j + 1 < len(onsets) else len(sig)\n",
        "                        s = self.find_zero_crossing(sig, s)\n",
        "                        e = self.find_zero_crossing(sig, e)\n",
        "\n",
        "                        dur = (e - s) / self.sr\n",
        "                        if not (cfg[\"min_duration\"] <= dur <= cfg[\"max_duration\"]):\n",
        "                            continue\n",
        "\n",
        "                        slice_y = self.apply_envelope(sig[s:e])\n",
        "                        y_fix = librosa.util.fix_length(slice_y, size=10240)\n",
        "\n",
        "                        row = {\n",
        "                            \"file_path\": path,\n",
        "                            \"channel\": ch,\n",
        "                            \"duration\": dur,\n",
        "                            \"start_sample\": int(s),\n",
        "                            \"end_sample\": int(e),\n",
        "                            \"rolloff\": 0.0,\n",
        "                            \"attack_slope\": 0.0,\n",
        "                            \"loudness\": float(np.mean(librosa.feature.rms(y=y_fix))),\n",
        "                        }\n",
        "\n",
        "                        # --- MFCC (mean only, 13 coeffs) ---\n",
        "                        mfcc = librosa.feature.mfcc(y=y_fix, sr=self.sr, n_mfcc=13)\n",
        "                        mfcc_d = librosa.feature.delta(mfcc)\n",
        "                        for idx in range(13):\n",
        "                            row[f\"mfcc_mean_{idx}\"] = float(np.mean(mfcc[idx]))\n",
        "                            row[f\"mfcc_delta_mean_{idx}\"] = float(np.mean(mfcc_d[idx]))\n",
        "\n",
        "                        # --- Spectral rolloff ---\n",
        "                        try:\n",
        "                            ro = librosa.feature.spectral_rolloff(y=y_fix, sr=self.sr)\n",
        "                            row[\"rolloff\"] = float(np.mean(ro)) if ro.size > 0 else 0.0\n",
        "                        except Exception:\n",
        "                            row[\"rolloff\"] = 0.0\n",
        "\n",
        "                        # --- Attack slope (first 50ms RMS) ---\n",
        "                        attack_len = int(0.05 * self.sr)\n",
        "                        attack = slice_y[:attack_len]\n",
        "                        if len(attack) > 8:\n",
        "                            rms_env = librosa.feature.rms(y=attack)[0]\n",
        "                            row[\"attack_slope\"] = float(np.polyfit(np.arange(len(rms_env)), rms_env, 1)[0])\n",
        "                        else:\n",
        "                            row[\"attack_slope\"] = 0.0\n",
        "\n",
        "                        rows.append(row)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "\n",
        "        self.df = pd.DataFrame(rows)\n",
        "        self.save_dataframe()\n",
        "        print(f\"‚ú® Extracted {len(self.df)} slices\")\n",
        "\n",
        "\n",
        "    def cluster(self, n_clusters=8, feature_weights=None, umap_config=None):\n",
        "        \"\"\"\n",
        "        Clusters slices based on core timbral features.\n",
        "        feature_weights: dict of feature_name -> multiplier (after scaling)\n",
        "        umap_config: dict with 'n_neighbors' and 'min_dist'\n",
        "        \"\"\"\n",
        "        if self.df.empty:\n",
        "            print(\"‚ö†Ô∏è No slices available for clustering\")\n",
        "            return\n",
        "\n",
        "        print(f\"üß† Clustering {len(self.df)} slices...\")\n",
        "\n",
        "        # ---- 1Ô∏è‚É£ Select core features ----\n",
        "        feature_cols = [f\"mfcc_mean_{i}\" for i in range(13)] + [\"loudness\", \"rolloff\"]\n",
        "        for col in feature_cols:\n",
        "            if col not in self.df.columns:\n",
        "                print(f\"‚ö†Ô∏è Missing {col}, filling with 0\")\n",
        "                self.df[col] = 0.0\n",
        "\n",
        "        X = self.df[feature_cols].values.astype(np.float32)\n",
        "\n",
        "        # ---- 2Ô∏è‚É£ Scale features ----\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "        # ---- 3Ô∏è‚É£ Apply weights if provided ----\n",
        "        if feature_weights:\n",
        "            weights = np.array([feature_weights.get(c, 1.0) for c in feature_cols], dtype=np.float32)\n",
        "            X_scaled *= weights\n",
        "\n",
        "        # ---- 4Ô∏è‚É£ UMAP embedding ----\n",
        "        umap_cfg = umap_config or {\"n_neighbors\": 15, \"min_dist\": 0.1}\n",
        "        reducer = umap.UMAP(n_neighbors=umap_cfg[\"n_neighbors\"],\n",
        "                            min_dist=umap_cfg[\"min_dist\"],\n",
        "                            metric='euclidean',\n",
        "                            random_state=42)\n",
        "        embedding = reducer.fit_transform(X_scaled)\n",
        "        self.df[\"umap_x\"], self.df[\"umap_y\"] = embedding[:, 0], embedding[:, 1]\n",
        "\n",
        "        # ---- 5Ô∏è‚É£ KMeans clustering ----\n",
        "        n_samples = len(self.df)\n",
        "        if n_clusters > n_samples:\n",
        "            n_clusters = n_samples\n",
        "            print(f\"‚ÑπÔ∏è Adjusted n_clusters to {n_samples} (number of slices)\")\n",
        "\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
        "        self.df[\"cluster\"] = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "        # ---- 6Ô∏è‚É£ Cluster summary ----\n",
        "        print(\"üìä Cluster distribution:\")\n",
        "        print(self.df.groupby(\"cluster\").size())\n",
        "\n",
        "      # ----------------\n",
        "      # Thinning logic: remove very similar slices after clustering\n",
        "      # ----------------\n",
        "\n",
        "    def thin(self, similarity_threshold=0.95, feature_cols=None, max_refs=300):\n",
        "        \"\"\"\n",
        "        Scalable greedy thinning using cosine similarity.\n",
        "        No NxN matrices. Safe for very large clusters.\n",
        "\n",
        "        similarity_threshold: float (0.9‚Äì0.99 typical)\n",
        "        max_refs: max number of reference slices per cluster\n",
        "                  (caps runtime + memory)\n",
        "        \"\"\"\n",
        "\n",
        "        if \"cluster\" not in self.df.columns:\n",
        "            raise RuntimeError(\"Run clustering before thinning\")\n",
        "\n",
        "        feature_cols = feature_cols or [c for c in self.df.columns if c.startswith(\"mfcc_\")]\n",
        "        print(f\"üîç Thinning with cosine similarity > {similarity_threshold}\")\n",
        "\n",
        "        kept_rows = []\n",
        "\n",
        "        for cid, cdf in self.df.groupby(\"cluster\"):\n",
        "            print(f\"  ‚Ä¢ Cluster {cid}: {len(cdf)} slices\")\n",
        "\n",
        "            X = cdf[feature_cols].values.astype(np.float32)\n",
        "            X = StandardScaler().fit_transform(X)\n",
        "\n",
        "            kept_idx = []\n",
        "            ref_vectors = []\n",
        "\n",
        "            for i, vec in enumerate(X):\n",
        "                if not ref_vectors:\n",
        "                    kept_idx.append(i)\n",
        "                    ref_vectors.append(vec)\n",
        "                    continue\n",
        "\n",
        "                sims = cosine_similarity(\n",
        "                    vec.reshape(1, -1),\n",
        "                    np.array(ref_vectors)\n",
        "                )[0]\n",
        "\n",
        "                if np.max(sims) < similarity_threshold:\n",
        "                    kept_idx.append(i)\n",
        "                    ref_vectors.append(vec)\n",
        "\n",
        "                    # cap reference size to avoid slowdown\n",
        "                    if len(ref_vectors) > max_refs:\n",
        "                        ref_vectors = ref_vectors[-max_refs:]\n",
        "\n",
        "            kept = cdf.iloc[kept_idx]\n",
        "            kept_rows.append(kept)\n",
        "\n",
        "            print(f\"    ‚úÇÔ∏è {len(cdf)} ‚Üí {len(kept)} kept\")\n",
        "\n",
        "        self.df = pd.concat(kept_rows).reset_index(drop=True)\n",
        "        print(f\"‚úÖ Final slice count: {len(self.df)}\")\n",
        "\n",
        "\n",
        "    def save_dataframe(self):\n",
        "        path = self.cfg[\"data_file\"]\n",
        "        print(f\"üíæ Saving analysis data to {path}\")\n",
        "\n",
        "        df = self.df.copy()\n",
        "        # df[\"raw_audio\"] = df[\"raw_audio\"].apply(lambda x: x.tolist())\n",
        "        df.to_csv(path, index=False)\n",
        "\n",
        "    def load_dataframe(self):\n",
        "        path = self.cfg[\"data_file\"]\n",
        "        print(f\"üìÇ Loading analysis data from {path}\")\n",
        "\n",
        "        df = pd.read_csv(path)\n",
        "        # df[\"raw_audio\"] = df[\"raw_audio\"].apply(lambda x: np.array(eval(x)))\n",
        "        self.df = df\n",
        "\n",
        "\n",
        "    def export(self):\n",
        "        out = self.cfg[\"output_folder\"]\n",
        "        os.makedirs(out, exist_ok=True)\n",
        "\n",
        "        print(f\"üíæ Writing slices to {out}\")\n",
        "\n",
        "\n",
        "        audio_cache = {}\n",
        "\n",
        "        for _, row in self.df.iterrows():\n",
        "            path = row[\"file_path\"]\n",
        "\n",
        "            if path not in audio_cache:\n",
        "                y, _ = librosa.load(path, sr=self.sr, mono=True)\n",
        "                audio_cache[path] = y\n",
        "            else:\n",
        "                y = audio_cache[path]\n",
        "\n",
        "            slice_y = y[row[\"start_sample\"]:row[\"end_sample\"]]\n",
        "            # Write to cluster folder\n",
        "            c_dir = os.path.join(self.cfg[\"output_folder\"], f\"Cluster_{row['cluster']}\")\n",
        "            os.makedirs(c_dir, exist_ok=True)\n",
        "            fn = f\"s_{_}_{row['channel']}.wav\"\n",
        "            sf.write(os.path.join(c_dir, fn), slice_y, self.sr)\n",
        "\n",
        "\n",
        "        print(\"‚úÖ Export complete\")\n",
        "\n"
      ],
      "metadata": {
        "id": "KYZ7jUCN3gsL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    if CONFIG[\"clear_existing\"]:\n",
        "        clear_existing_state(CONFIG)\n",
        "\n",
        "    brain = SampleBrain(CONFIG)\n",
        "\n",
        "    brain.analyze()   # load or compute\n",
        "    brain.cluster()   # fast, repeatable\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dg9XIieb8r2a",
        "outputId": "104b6640-074e-4674-b9be-8d7890f9aa31"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÇ Loading analysis data from /content/drive/My Drive/audio/sp-tools-corpora/preparedpiano/clusters/analysis_data.csv\n",
            "‚úÖ Loaded 91570 existing slices\n",
            "üß† Clustering 91570 slices...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä Cluster distribution:\n",
            "cluster\n",
            "0    31608\n",
            "1    10491\n",
            "2     8728\n",
            "3     8433\n",
            "4     8477\n",
            "5     9712\n",
            "6     6712\n",
            "7     7409\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brain.thin(.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ume9DlGzXxDj",
        "outputId": "9143f3a7-0b30-4c7f-f1fe-63d5159037ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Thinning with cosine similarity > 0.3\n",
            "  ‚Ä¢ Cluster 0: 31608 slices\n",
            "    ‚úÇÔ∏è 31608 ‚Üí 40 kept\n",
            "  ‚Ä¢ Cluster 1: 10491 slices\n",
            "    ‚úÇÔ∏è 10491 ‚Üí 39 kept\n",
            "  ‚Ä¢ Cluster 2: 8728 slices\n",
            "    ‚úÇÔ∏è 8728 ‚Üí 49 kept\n",
            "  ‚Ä¢ Cluster 3: 8433 slices\n",
            "    ‚úÇÔ∏è 8433 ‚Üí 45 kept\n",
            "  ‚Ä¢ Cluster 4: 8477 slices\n",
            "    ‚úÇÔ∏è 8477 ‚Üí 47 kept\n",
            "  ‚Ä¢ Cluster 5: 9712 slices\n",
            "    ‚úÇÔ∏è 9712 ‚Üí 51 kept\n",
            "  ‚Ä¢ Cluster 6: 6712 slices\n",
            "    ‚úÇÔ∏è 6712 ‚Üí 36 kept\n",
            "  ‚Ä¢ Cluster 7: 7409 slices\n",
            "    ‚úÇÔ∏è 7409 ‚Üí 27 kept\n",
            "‚úÖ Final slice count: 334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brain.export()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ULJdv1vXzrd",
        "outputId": "8a0b8553-637c-4408-c339-31a6f5966666"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Writing slices to /content/drive/My Drive/audio/sp-tools-corpora/preparedpiano/clusters\n",
            "‚úÖ Export complete\n"
          ]
        }
      ]
    }
  ]
}